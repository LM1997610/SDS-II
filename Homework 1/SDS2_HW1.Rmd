---
title: "Statstical Methods in Data Science II"
date: "A.Y. 2023-2024"
subtitle: "SDS II - Homework 1"
author: "Luca Mazzucco: 1997610"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A) Simulation:
Joint probability distribution:

```{r load data, echo=FALSE}

load("Hmwk.RData")
print(J)

```

In order to check that joint probability distribution J is a probability distribution:\

- **Non-negativity**: all probabilities in the distribution must be non-negative\
- **Summation to 1**: sum of all probabilities over all the possible outcomes is equal to 1


```{r ex_1.1}

P_y <- rowSums(J) # marginal of Y
P_z <- colSums(J) # marginal of Z

c(sum(P_y) == 1, sum(P_z) == 1)  # marginal of Y, marginal of Z

```
From the joint distribution J can be derived *six conditional distributions*:\

One for each event Y given Z (z={1,2,3}).

```{r ex_1.2, echo=FALSE}

J[, 1]/P_z[1]  # Pr(Y\Z=1)
J[, 2]/P_z[2]  # Pr(Y\Z=2)
J[, 3]/P_z[3]  # Pr(Y\Z=3)

```

And same for Z given Y (y={1,2,3})
```{r ex_1.21, echo=FALSE}

J[1, ]/P_y[1]  # Pr(Y\Z=1)
J[2, ]/P_y[2]  # Pr(Y\Z=2)
J[3, ]/P_y[3]  # Pr(Y\Z=3)

```


Make sure they are probability distributions:

```{r ex_1.3}

c(sum(J[, 1]/P_z[1]), sum(J[, 2]/P_z[2]), sum(J[, 3]/P_z[3])) # each distrib sum up to one
c(sum(J[1, ]/P_y[1]), sum(J[2, ]/P_y[2]), sum(J[3, ]/P_y[3]))

```

Simulate from this J distribution:

```{r ex_1.4}

events_y <- 1:3
events_z <- 1:3

n_simulations <- 100
simulated_data <- numeric(n_simulations)

for (i in 1:n_simulations) {
  y <- sample(events_y, 1, prob = rowSums(J))
  z <- sample(events_z, 1, prob = J[y, ])
  simulated_data[i] <- z}

# simulated_data

```


## B) **Bulbs lifetime**: a conjugate Bayesian analysis of exp data:

```{r ex_2.1}

y_obs <- c(1, 13, 27, 43, 73, 75, 154, 196, 220, 297, 344, 610, 734,    #Bulbs lifetime data (in hours)
                                  783, 796, 845, 859, 992, 1066, 1471)

```

```{r ex_2.1_plot, echo = FALSE}

hist(y_obs, main = "Distribution of Bulbs Lifetime", xlab = "hours", breaks = 20)

```


Main ingredients of the **Bayesian model** are:\

- **Assumption** on the **statistical model**: that represent our beliefs about the probability distribution of the          observed data Y, given a specific parameter θ.\
  Exponential model → $Y_i \vert \theta \sim \text{Exp}(\theta)$
 
- **Prior distribution** - π(θ) of the unknown parameter of interest → $\theta \sim Gamma(r,s)$\
    Here used the gamma distribution, which is a conjugate prior distribution for the exponential model. 
 
- **Posterior distribution** π(θ∣y) → what we are really interested, it's output of the model, used to make inference\
  Updated based on the observed data through the **Bayes rule**

Choose a conjugate prior distribution π(θ):
```{r ex_2.20}

mean = 0.003
std = 0.00173

r_prior <- mean/std^2
s_prior <- mean^2/std^2
```

```{r ex_2.21, echo=FALSE}
cat("Prior parameters of the gamma distribution: r =", r_prior, "s =", s_prior)
```

```{r ex_2.22}
sum_y <- sum(y_obs) 
n <- length(y_obs)

r_post <- r_prior + sum_y 
s_post <- s_prior + n

```


```{r ex_2.23, echo=FALSE}
cat("Post parameters of the gamma distribution: r =", r_post, "s =", s_post)
```

```{r ex_2.2 plot, echo = FALSE}

par(mfrow=c(1,1))
curve(dgamma(x, rate=r_prior, shape=s_prior), from=0, to=0.015,main="", xlab=expression(theta), ylab= "", col="#008000", ylim= c(0, 1000), lwd=2.5)
curve(dgamma(x, rate=r_post, shape=s_post), from=0, to=0.015, main="", xlab=expression(theta), ylab= "", col="#90EE90", add=TRUE, ylim= c(0, 1000), lwd=2)
title(main="Prior and Posterior Distributions", cex.main=1.2)
legend("topright", legend=c("Prior Distribution", "Posterior Distribution"), col=c("#008000", "#90EE90"), lwd=2.5, cex=0.65)
grid()

```

**Point estimation** analysis on θ:

```{r ex_2.3, echo = FALSE}

post_median <- qgamma(0.5, shape=s_post, rate=r_post)  # posterior theta median 
post_mean <- s_post/r_post                             # posterior theta mean 

cat("posterior median =", post_median)
cat("posterior mean =", post_mean)
cat("posterior var =", format(s_post/(r_post^2), scientific = F))

prior_mean = s_prior/r_prior       # prior theta mean 
prior_var = s_prior/(r_prior^2)    # prior theta var

cat("prior_mean =", prior_mean, "\nprior_var =", format(prior_var, scientific = F))

```

Variance from  Posterior distribution is much lower than Prior variance. Uncertainty in Prior was reduced thanks to the observation of the data.\
This explains the Prior distribution is vaguer than the Posterior, because it captures a wider uncertainty\
Also the mean has been lowered,  moving from 0.003 to 0.0021 in Posterior distribution.

\

Relevant information learnt about the average lifetime of the bulb (ψ = 1/θ):

```{r ex_2.5, echo = FALSE}

cat("Posterior statistics on bulbs lifetime:")

cat("median =", 1/post_median)
cat("mean =", 1/post_mean, "→ hours average lifetime")

cat("mean of observated data =", mean(y_obs))

```

The posterior mean shows average lifetime is equal to 460.78 hours and corresponds quite well to the average of the observed data.\

Comparing the Prior and Posterior distributions, the average lifetime of bulbs increased:

```{r ex_2.5.1, echo = FALSE}

cat("prior_mean =", 1/prior_mean, "hours")

```

\

Finding the probability that the **average bulb lifetime exceeds 550 hours**.\
Using the cumulative distribution function from the posterior gamma distribution learned before:\

```{r ex_2.6, echo = TRUE}

avg_value <- pgamma(1/550, shape = s_post, rate = r_post)  # ψ = 1/θ

```

```{r ex_2.61, echo = FALSE}

cat("the probability of a bulb with lifetime greater than 550 is :", avg_value)

```

## C) Exchangeability

Given a sequence of random variables \( X_1,\ldots, X_n,\ldots\) it is **exchangeable** if\
for any \( k\)-tuple \( (n_1, \ldots, n_k)\) and any permutation\(\sigma = (\sigma_1,\ldots, \sigma_k)\) of the first \( k\) integers, the following holds:\
\[(X_{n_1}, \ldots, X_{n_k}) \stackrel{d}{=} (X_{n_{\sigma_1}}, \ldots, X_{n_{\sigma_k}})\]

From the **De Finetti theorem**:

If \(X_1, \ldots, X_n, \ldots\) is an exchangeable process of binary random variables, there exists a distribution \(\pi\) on [0, 1] such that
\[\text{Pr}(X_1 = x_1,\ldots, X_n = x_n) = \int_{0}^{1} \prod_{i=1}^{n} \theta^{x_i} (1 - \theta)^{1-x_i} \pi(\theta) d\theta\]
Where the random variables are conditionally independent and identically distributed.

